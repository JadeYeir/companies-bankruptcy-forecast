---
title: "R Notebook"
output: html_notebook
---

```{r}
library(dplyr)
library(tidyverse)
library(rpart)
library(statisticalModeling)
library(permute)
library(rpart.plot)
set.seed(42)
```


```{r}
training_data <- select(read_csv('../data/bankruptcy_train_am.csv'), -X1)
test_data <- select(read_csv('../data/bankruptcy_test_am.csv'), -X1)
```

```{r}
head(training_data)
```
```{r}
head(test_data)
```


### Pre-process - factor and scale data



```{r}
training_data$class <- factor(training_data$class, levels = c(0,1))
test_data$class <- factor(test_data$class, levels = c(0,1))
```


```{r}
head(training_data)
head(test_data)
```


## NULL MODEL!
```{r}
training_data_null <- training_data
training_data_null$all_the_same <- 1 # null "explanatory" variable
```

```{r}
head(training_data_null)
```


```{r}
null_model <- rpart(class ~ all_the_same, data=training_data_null)
```


Evaluate the null model on training data
```{r}
null_model_output <- evaluate_model(null_model, data = training_data_null, type="class")
```


Calculate the error rate
```{r}
with(data = null_model_output, mean(class != model_output))
```

Generate a random guess...
```{r}
null_model_output$random_guess <- shuffle(null_model_output$class)
with(data=null_model_output, mean(class != null_model_output$random_guess))
```

Null model with test data
```{r}
test_data_null <- test_data
test_data_null$all_the_same <- 1
null_model_output <- evaluate_model(null_model, data = test_data_null, type="class")
with(data=null_model_output, mean(class != model_output))

```


## ALL VARS MODEL

```{r}
model_all <- rpart(class ~ ., data = training_data, cp = 0.001)
```

in-sample validation
```{r}
model_all_output <- evaluate_model(model_all, data = training_data, type = "class")
table(model_all_output$class, model_all_output$model_output)
```
```{r}
with(data = model_all_output, mean(class != model_output, na.rm = TRUE))

```


Test data validation

```{r}
model_all_output_test <- evaluate_model(model_all, data = test_data, type = "class")
table(test_data$class, model_all_output_test$model_output)
```


```{r}
with(data = model_all_output_test, mean(class != model_output, na.rm = TRUE))
```

```{r}
prp(model_all, type = 3)
```


## EXPERT MODEL
Attr8 + Attr10 + Attr12 + Attr20 + Attr33 + Attr40 + Attr42 + Attr46 + Attr49 + Attr59 + Attr63 + Attr64
```{r}
model_expert <- rpart(class ~ Attr8 + Attr10 + Attr12 + Attr20 + Attr33 + Attr40 + Attr42 + Attr46 + Attr49 + Attr59 + Attr63 + Attr64, data=training_data, cp=0.001 )
```

in-sample validation
```{r}
model_expert_output <- evaluate_model(model_expert, data = training_data, type = "class")
table(model_expert_output$model_output)
```


```{r}
with(data = model_expert_output, mean(class != model_output, na.rm = TRUE))

```


Test data validation

```{r}
model_expert_output_test <- evaluate_model(model_expert, data = test_data, type = "class")
table(model_expert_output_test$model_output)
```


```{r}
with(data = model_expert_output_test, mean(class != model_output, na.rm = TRUE))
```

```{r}
table(model_expert_output_test$class, model_expert_output_test$model_output)
```


```{r}
prp(model_expert, type = 3)
```

In-sample validation
```{r}
model_expert_output_prob <- evaluate_model(model_expert, data = training_data, type = "prob")
```


```{r}
dfm_insample <- data.frame(model_expert_output_prob$class, model_expert_output_prob$model_output)
```


```{r}
colnames(dfm_insample) <- c('class', 'p0', 'p1')
```

```{r}
summary(filter(dfm_insample, dfm_insample$class==1))
```
```{r}
dfm_insample$scaled_class <- ifelse(dfm_insample$p1>0.14, 1, 0)
```
 
```{r}
table(dfm_insample$class, dfm_insample$scaled_class)
```
```{r}
with(data = dfm_insample, mean(class != scaled_class, na.rm = TRUE))
```




```{r}
model_expert_output_prob$scaled_class <- ifelse(model_expert_output_prob$model_output[,2] > 0.14, 1 , 0)
```

```{r}
table(model_expert_output_prob$class, model_expert_output_prob$scaled_class)
```

```{r}
with(data = model_expert_output_prob, mean(class != scaled_class, na.rm = TRUE))
```


```{r}
plot(model_expert_output_prob$model_output[,2], model_expert_output_prob$class)
```



### Test Prob-Expert

```{r}
model_expert_output_test_prob <- evaluate_model(model_expert, data = test_data, type = "prob")
head(model_expert_output_test_prob)

```

```{r}
model_expert_output_prob_test$scaled_class <- ifelse(model_expert_output_prob_test$model_output[,2] > 0.14, 1 , 0)
```


```{r}
table(model_expert_output_prob_test$class, model_expert_output_prob_test$scaled_class)
```


```{r}
with(data = model_expert_output_prob_test, mean(class != scaled_class, na.rm = TRUE))
```





## CONTINUE WITH ALL MODEL ANALYSIS

### MODEL_1
```{r}
model_1 <- rpart(class ~ Attr24 + Attr25 + Attr26, data = training_data )
```


In-sample validation
```{r}
model_1_output<-evaluate_model(model_1, data=training_data, type='class')
cat("Confusion Matrix: ") 
table(model_1_output$class, model_1_output$model_output)
cat("\n Accuracy: ", with(data = model_1_output, mean(class != model_output, na.rm = TRUE)))
```

Test 
```{r}
model_1_output_test<-evaluate_model(model_1, data=test_data, type='class')
cat("Confusion Matrix: ") 
table(model_1_output_test$class, model_1_output_test$model_output)
cat("\n Accuracy: ", with(data = model_1_output_test, mean(class != model_output, na.rm = TRUE)))
```



### MODEL_2
```{r}
model_2 <- rpart(class ~ Attr24 + Attr25 + Attr26 + Attr21 + Attr34 + Attr5 , data = training_data )
```


In-sample validation
```{r}
model_2_output<-evaluate_model(model_2, data=training_data, type='class')
cat("Confusion Matrix: ") 
table(model_2_output$class, model_2_output$model_output)
cat("\n Accuracy: ", with(data = model_2_output, mean(class != model_output, na.rm = TRUE)))
```


Test 
```{r}
model_2_output_test<-evaluate_model(model_2, data=test_data, type='class')
cat("Confusion Matrix: ") 
table(model_2_output_test$class, model_2_output_test$model_output)
cat("\n Accuracy: ", with(data = model_2_output_test, mean(class != model_output, na.rm = TRUE)))
```


### MODEL_3
Adding 46 IMPROVES HOWEVER ADDING 56 & 41 DIDN'T CHANGE
```{r}
model_3 <- rpart(class ~ Attr24 + Attr25 + Attr26 + Attr21 + Attr34 + Attr5 + Attr46, data = training_data )
```


In-sample validation
```{r}
model_3_output<-evaluate_model(model_3, data=training_data, type='class')
cat("Confusion Matrix: ") 
table(model_3_output$class, model_3_output$model_output)
cat("\n Accuracy: ", with(data = model_3_output, mean(class != model_output, na.rm = TRUE)))
```


Test 
```{r}
model_3_output_test<-evaluate_model(model_3, data=test_data, type='class')
cat("Confusion Matrix: ") 
table(model_3_output_test$class, model_3_output_test$model_output)
cat("\n Accuracy: ", with(data = model_3_output_test, mean(class != model_output, na.rm = TRUE)))
```

### MODEL_4 : GLM
Adding 46 IMPROVES HOWEVER ADDING 56 & 41 DIDN'T CHANGE
```{r}
model_4 <- glm(class ~ Attr24 + Attr25 + Attr26 + Attr21 + Attr34 + Attr5 + Attr46, data = training_data, family=binomial(link="logit"))
```


In-sample validation
```{r}
model_4_output<-predict(model_4, data=training_data, type="response")
model_4_output <- as.integer(model_4_output >= 0.5)
cat("Confusion Matrix: ") 
table(training_data$class, model_4_output)
#cat("\n Accuracy: ", with(data = model_4_output, mean(class != model_output, na.rm = TRUE)))
```


Test 
```{r}
model_4_output_test<-predict(model_4, newdata = test_data, type='response')
model_4_output_test <- as.integer(model_4_output_test >= 0.5)
cat("Confusion Matrix: ") 
table(test_data$class, model_4_output_test)
#cat("\n Accuracy: ", with(data = model_3_output_test, mean(class != model_output, na.rm = TRUE)))
```

Effect Size

```{r}
library(caret)
varImp(model_4)
```




