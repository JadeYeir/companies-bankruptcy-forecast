---
title: "R Notebook"
output: html_notebook
pdf_document:
    keep_tex: true
---

# USE SMOTE

```{r}
library(tidyverse)
library(caret)
library(GGally)
library(ggplot2)
library(corrplot)
library(bayesplot)
theme_set(bayesplot::theme_default(base_family = "sans"))
library(rstanarm)
options(mc.cores = parallel::detectCores())
library(loo)
library(projpred)

library(bayestestR)
library("report")
library("see")
library("parameters")

library("statisticalModeling")

library(DMwR)
SEED=42
set.seed(42)
```


```{r}
training_data <- as.data.frame(select(read_csv('../data/bankruptcy_train_am.csv'), -X1))
test_data <- select(read_csv('../data/bankruptcy_test_am.csv'), -X1)
```


### Apply SMOTE AND Pre-process - factor and normalize data

```{r}
training_data$class <- factor(training_data$class, levels = c(0,1))
test_data$class <- factor(test_data$class, levels = c(0,1))
```

```{r}
table(training_data$class)
```


```{r}
training_data_smt <- SMOTE(class ~ ., training_data, perc.over = 400, perc.under = 200)
table(training_data_smt$class)
```


```{r}
for (i in 1:64) {
      training_data_smt[i] <- scale(training_data_smt[i], center = TRUE, scale = TRUE)
      test_data[i] <- scale(test_data[i], center = TRUE, scale = TRUE)
}
```


```{r}
head(training_data_smt)
```

```{r}
dim(test_data)
```


```{r}
table(training_data_smt$class)
```


```{r}
corrplot(cor(training_data_smt[, c(65,1:64)]))
```


## Build rstanarm model with selected variables RPART

```{r}
t_prior <- student_t(df = 7, location = 0, scale = 2.5)

model_bayes_smt <- stan_glm(class ~ Attr24 + Attr25 + Attr26 + Attr34 + Attr5 + Attr46,
                 family = binomial(link = "logit"), data = training_data_smt,
                 prior = t_prior, prior_intercept = t_prior, QR=TRUE,cores=4,
                 seed = 42)

```


```{r}
summary(model_bayes_smt)
```

```{r}
describe_posterior(model_bayes_smt)
```


## REPORT

```{r}
report(model = model_bayes_smt)
```


```{r}
pplot<-plot(model_bayes_1, "areas", prob = 0.95, prob_outer = 1)
pplot+ geom_vline(xintercept = 0)
```


## Expert Model
```{r}
model_expert_smt <- stan_glm(class ~ Attr8 + Attr10 + Attr12 + Attr20 + Attr33 + Attr40 + Attr42 + Attr46 + Attr49 + Attr59 + Attr63 + Attr64, family = binomial(link = "logit"), data = training_data_smt,
                 prior = t_prior, prior_intercept = t_prior, QR=TRUE,cores=4,
                 seed = 42) 
       
```

### Save Bayesian Expert Model
```{r}
#write_rds(model_expert_smt, "../model/model_expert_smt.rds")

```


```{r}
summary(model_expert_smt)
```


```{r}
report(model_expert)
```


```{r}
describe_posterior(model_expert_smt)
```

```{r}
equivalence_test(model_expert)
```


```{r}
p_direction(model_expert)
```


## SEE Visualization

```{r}
result_pd <- p_direction(model_expert)
```

```{r}
print(result_pd)
```
```{r}
plot(result_pd)
```


```{r}
result <- estimate_density(model_expert)
```

```{r}
plot(result)
```
```{r}
plot(result, stack = FALSE)
```

```{r}
plot(result, stack = FALSE, priors = TRUE)
```


```{r}
result <- p_direction(model_expert, effects = "all", component = "all")
result
```


```{r}
plot(result, n_columns=NULL)
```

```{r}
result <- p_significance(model_expert, effects = "all", component = "all")
```

```{r}
plot(result)
```

### Point estimates
```{r}
result <- point_estimate(model_expert)
plot(result)
```

```{r}
result <- equivalence_test(model_expert)
```


```{r}
plot(result) +
  theme_blackboard() +
  scale_fill_material()
```



```{r}
result <- model_parameters(model_expert,  effects = "all", component = "all")

plot(result)
```


```{r}
corrplot(training_data)
```



```{r}

pplot<-plot(model_expert, "areas", prob = 0.95, prob_outer = 1)
pplot+ geom_vline(xintercept = 0)
```

## Compare Models
```{r}
loo_bayes_1 <- loo(model_bayes_smt)
```


```{r}
lkfold_bayes_1 <- kfold(model_bayes_1, K = 10)
```

```{r}
lkfold_expert <- kfold(model_expert, K = 10)
```

```{r}
loo_compare(lkfold_bayes_1, lkfold_expert)
```

```{r}
loo_compare(lkfold_expert, lkfold_bayes_1)
```

Interpretation:https://mc-stan.org/rstanarm/articles/binomial.html
These results favor model_expert over model_bayes_1, as the estimated difference in elpd (the expected log pointwise predictive density for a new dataset) is so much larger than its standard error. LOO penalizes models for adding additional predictors (this helps counter overfitting).

Overall : https://avehtari.github.io/modelselection/diabetes.html



```{r}
lkfold_expert
```


```{r}
lkfold_bayes_1
kfol
```


```{r}
```


## Prior Summary
```{r}
prior_summary(model_bayes_1)
```

```{r}
round(posterior_interval(model_bayes_1, prob = 0.9), 2)
```

## Predictive performance

```{r}
# Predicted probabilities
linpred <- posterior_linpred(model_bayes_smt)
preds <- posterior_linpred(model_bayes_smt, transform=TRUE)
pred <- colMeans(preds)
pr <- as.integer(pred >= 0.5)
   
# posterior classification accuracy
round(mean(xor(pr,as.integer(training_data_smt$class==0))),2)

```


```{r}
# posterior balanced classification accuracy
round((mean(xor(pr[training_data_smt$class==0]>0.5,as.integer(training_data_smt$class[training_data_smt$class==0])))+mean(xor(pr[training_data_smt$class==1]<0.5,as.integer(training_data_smt$class[training_data_smt$class==1]))))/2,2)
```

## Test

### In-sample Validation Bayesian - rpart

```{r}
preds_insample_smt <- posterior_linpred(model_bayes_smt, transform=TRUE)
pred_insample_smt <- colMeans(preds_insample_smt)
```

```{r}
pr_insample_smt <- as.integer(pred_insample_smt >= 0.5)
   
# posterior classification accuracy
round(mean(xor(pr_insample_smt,as.integer(training_data_smt$class==0))),2)

table(training_data_smt$class, pr_insample_smt)
```


```{r}
```

### Test - Bayesian - rpart
```{r}
preds_test_smt <- posterior_predict(model_bayes_smt, newdata = test_data)
pred_test_smt <- colMeans(preds_test_smt)
```


```{r}
pr_test_smt <- as.integer(pred_test_smt >= 0.5)
   
# posterior classification accuracy
round(mean(xor(pr_test_smt,as.integer(test_data$class==0))),2)

table(test_data$class, pr_test_smt)
```

### In-sample Validation Bayesian - EXPERT

```{r}
preds_insample_smt <- posterior_linpred(model_expert_smt, transform=TRUE)
pred_insample_smt <- colMeans(preds_insample_smt)
```

```{r}
pr_insample_smt <- as.integer(pred_insample_smt >= 0.5)
   
# posterior classification accuracy
round(mean(xor(pr_insample_smt,as.integer(training_data_smt$class==0))),2)

table(training_data_smt$class, pr_insample_smt)
```


### Test - Bayesian - EXPERT
```{r}
preds_test_smt <- posterior_predict(model_expert_smt, newdata = test_data)
pred_test_smt <- colMeans(preds_test_smt)
```


```{r}
pr_test_smt <- as.integer(pred_test_smt >= 0.5)
   
# posterior classification accuracy
round(mean(xor(pr_test_smt,as.integer(test_data$class==0))),3)

table(test_data$class, pr_test_smt)
```
