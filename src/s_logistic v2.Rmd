---
title: "R Notebook"
output: html_notebook
---

```{r}
library(dplyr)
library(tidyverse)
set.seed(42)
```


```{r}
training_data <- select(read_csv('../data/bankruptcy_train_am.csv'), -X1)
test_data <- select(read_csv('../data/bankruptcy_test_am.csv'), -X1)
```

```{r}
head(training_data)
```
```{r}
head(test_data)
```


### Pre-process - factor and scale data



```{r}
training_data$class <- factor(training_data$class, levels = c(0,1))
test_data$class <- factor(test_data$class, levels = c(0,1))
```


```{r}
head(training_data)
head(test_data)
```



```{r}
logitMod <- glm(class ~ ., data=training_data, family=binomial(link="logit"))

```


### In sample validation

```{r}
predicted_in <- predict(logitMod, training_data, type="response")
predicted_scaled_in <- as.integer(predicted_in >= 0.5)
```


```{r}
table(training_data$class, predicted_scaled_in)
MLmetrics::Accuracy(predicted_scaled_in, training_data$class)
MLmetrics::Precision(training_data$class, predicted_scaled_in)
MLmetrics::Recall(training_data$class, predicted_scaled_in)
MLmetrics::F1_Score(training_data$class, predicted_scaled_in)
```

### Test validation
```{r}
predicted <- predict(logitMod, test_data, type="response")
```

```{r}
length(predicted)
```

```{r}
predicted_scaled <- as.integer(predicted >= 0.5)
```

```{r}
table(predicted_scaled)
```

```{r}
true_pr <- test_data$class
table(true_pr, predicted_scaled)

MLmetrics::Accuracy(predicted_scaled, test_data$class)
MLmetrics::Precision(test_data$class, predicted_scaled)
MLmetrics::Recall(test_data$class, predicted_scaled)
MLmetrics::F1_Score(test_data$class, predicted_scaled)
```





## GLM With Expert
```{r}
logitMod_expert <- glm(class ~ Attr8 + Attr10 + Attr12 + Attr20 + Attr33 + Attr40 + Attr42 + Attr46 + Attr49 + Attr59 + Attr63 + Attr64, data=training_data, family=binomial(link="logit"))
```

```{r}
predicted_expert <- predict(logitMod_expert, test_data, type="response")
```

```{r}
length(predicted_expert)
```

```{r}
predicted_scaled_expert <- as.integer(predicted_expert >= 0.5)
````


```{r}

table(test_data$class, predicted_scaled_expert)
MLmetrics::Accuracy(predicted_scaled_expert, test_data$class)
MLmetrics::Precision(test_data$class, predicted_scaled_expert)
MLmetrics::Recall(test_data$class, predicted_scaled_expert)
MLmetrics::F1_Score(test_data$class, predicted_scaled_expert)
```




## Hypothesis testing
```{r}
summary(logitMod)
```



### To get the significance for the overall model we use the following command:
https://sphweb.bumc.bu.edu/otlt/MPH-Modules/BS/R/R7_LogisticRegression-Survival/R7_LogisticRegression-Survival3.html

```{r}
1-pchisq(1105.8-10308.5, 5599-5536)
```

```{r}
summary(logitMod)
```


```{r}
library(report)
```

```{r}
report(logitMod)
```


## Use of Evaluate_Model and other facelts from statisticalModeling package
```{r}
library(statisticalModeling)
```


```{r}
eval_pred <- evaluate_model(logitMod, test_data[,-65])
```

```{r}
eval_pred$model_output <- as.integer(eval_pred$model_output >= 0.5)
```

```{r}
table(eval_pred$model_output)
```

```{r}
eval_pred$true_class <- test_data$class
```

```{r}
filter(eval_pred, eval_pred$model_output != eval_pred$true_class)
```
```{r}
table(eval_pred$model_output, eval_pred$true_class)
```

```{r}
evaluate_model(logitMod)
```


