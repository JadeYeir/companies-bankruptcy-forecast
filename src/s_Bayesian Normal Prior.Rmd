---
title: "Generalized Linear Models using rstanarm"
output: html_notebook
---

## Generalized Linear Models
Generalized Linear Models (GLM) are similar to linear regression models. Here instead of having the normally distributed output variable it could be constrained, for example, for binary logistic regression, the output or response variable can be made to restrict between 0 and 1.



```{r}
library(tidyverse)
library(caret)
library(GGally)
library(ggplot2)
library(corrplot)
library(bayesplot)
theme_set(bayesplot::theme_default(base_family = "sans"))
library(rstanarm)
options(mc.cores = parallel::detectCores())
library(loo)
library(projpred)

library(bayestestR)
library("report")
SEED=42
set.seed(42)
```


```{r}
training_data <- select(read_csv('../data/bankruptcy_train_am.csv'), -X1)
test_data <- select(read_csv('../data/bankruptcy_test_am.csv'), -X1)
```


### Pre-process - factor and normalize data

```{r}
training_data$class <- factor(training_data$class, levels = c(0,1))
test_data$class <- factor(test_data$class, levels = c(0,1))
```

```{r}
for (i in 1:64) {
      training_data[i] <- scale(training_data[i], center = TRUE, scale = TRUE)
      test_data[i] <- scale(test_data[i], center = TRUE, scale = TRUE)
}
```


```{r}
table(training_data$class)
```

```{r}
table(test_data$class)
```


```{r}
dim(test_data)
```


```{r}
table(training_data$class)
```


## Build rstanarm model with selected variables RPART - With Normal prior/default priors

```{r}
model_bayes_norm <- stan_glm(class ~ Attr24 + Attr25 + Attr26 + Attr34 + Attr5 + Attr46,
                 family = binomial(link = "logit"), data = training_data,
                 QR=TRUE,cores=4,
                 seed = 42)

```


```{r}
summary(model_bayes_norm)
```

```{r}
describe_posterior(model_bayes_1)
```


## REPORT

```{r}
report(model = model_bayes_1)
```


```{r}
pplot<-plot(model_bayes_1, "areas", prob = 0.95, prob_outer = 1)
pplot+ geom_vline(xintercept = 0)
```


## Expert Model
```{r}
model_expert <- stan_glm(class ~ Attr8 + Attr10 + Attr12 + Attr20 + Attr33 + Attr40 + Attr42 + Attr46 + Attr49 + Attr59 + Attr63 + Attr64, family = binomial(link = "logit"), data = training_data,
                 prior = t_prior, prior_intercept = t_prior, QR=TRUE,cores=4,
                 seed = 42) 
       
```


```{r}
summary(model_expert)
```


```{r}
report(model_expert)
```


```{r}
describe_posterior(model_expert)
```

```{r}

pplot<-plot(model_expert, "areas", prob = 0.95, prob_outer = 1)
pplot+ geom_vline(xintercept = 0)
```

## Compare Models
```{r}
loo_bayes_1 <- loo(model_bayes_1)
```


```{r}
lkfold_bayes_1 <- kfold(model_bayes_1, K = 10)
```

```{r}
lkfold_expert <- kfold(model_expert, K = 10)
```
```{r}
loo_compare(lkfold_bayes_1, lkfold_expert)
```

```{r}
loo_compare(lkfold_expert, lkfold_bayes_1)
```

Interpretation:https://mc-stan.org/rstanarm/articles/binomial.html
These results favor fit2 over model_bayes_1, as the estimated difference in elpd (the expected log pointwise predictive density for a new dataset) is so much larger than its standard error. LOO penalizes models for adding additional predictors (this helps counter overfitting).

Overall : https://avehtari.github.io/modelselection/diabetes.html


## Prior Summary
```{r}
prior_summary(model_bayes_1)
```
```{r}
round(posterior_interval(model_bayes_1, prob = 0.9), 2)
```

## Predictive performance

```{r}
# Predicted probabilities
linpred <- posterior_linpred(model_bayes_1)
preds <- posterior_linpred(model_bayes_1, transform=TRUE)
pred <- colMeans(preds)
pr <- as.integer(pred >= 0.5)
   
# posterior classification accuracy
round(mean(xor(pr,as.integer(training_data$class==0))),2)

```


```{r}
# posterior balanced classification accuracy
round((mean(xor(pr[training_data$class==0]>0.5,as.integer(training_data$class[training_data$class==0])))+mean(xor(pr[training_data$class==1]<0.5,as.integer(training_data$class[training_data$class==1]))))/2,2)
```

## Test

### In-sample Validation Bayesian - rpart

```{r}
preds_insample_1 <- posterior_linpred(model_bayes_1, transform=TRUE)
pred_insample_1 <- colMeans(preds_insample_1)
```

```{r}
pr_insample_1 <- as.integer(pred_insample_1 >= 0.09)
   
# posterior classification accuracy
round(mean(xor(pr_insample_1,as.integer(training_data$class==0))),2)

table(training_data$class, pr_insample_1)
```


```{r}
```

### Test - Bayesian - rpart
```{r}
preds_test_1 <- posterior_predict(model_bayes_1, newdata = test_data)
pred_test_1 <- colMeans(preds_test_1)
```


```{r}
pr_test_1 <- as.integer(pred_test_1 >= 0.09)
   
# posterior classification accuracy
round(mean(xor(pr_test_1,as.integer(test_data$class==0))),2)

table(test_data$class, pr_test_1)
```

### In-sample Validation Bayesian - EXPERT

```{r}
preds_insample <- posterior_linpred(model_expert, transform=TRUE)
pred_insample <- colMeans(preds_insample)
```

```{r}
pr_insample <- as.integer(pred_insample >= 0.1)
   
# posterior classification accuracy
round(mean(xor(pr_insample,as.integer(training_data$class==0))),2)

table(training_data$class, pr_insample)
```


### Test - Bayesian - EXPERT
```{r}
preds_test <- posterior_predict(model_expert, newdata = test_data)
pred_test <- colMeans(preds_test)
```


```{r}
pr_test <- as.integer(pred_test >= 0.5)
   
# posterior classification accuracy
round(mean(xor(pr_test,as.integer(test_data$class==0))),3)

table(test_data$class, pr_test)
```


```{r}
report(model_expert)
```


```{r}
rstan::get_stanmodel(model_bayes_1$stanfit)
```

